"""
å›¾ç‰‡å¤„ç†æ¨¡å—

æ”¯æŒå¤šç§å›¾ç‰‡è¾“å…¥æ ¼å¼çš„å¤„ç†ï¼š
- æœ¬åœ°æ–‡ä»¶è·¯å¾„
- ç½‘ç»œURL
"""

from __future__ import annotations
import asyncio
import os
import tempfile
from pathlib import Path
from typing import List, Union, Optional, Tuple
import uuid
import aiohttp
from .logger import get_logger
from ..core.playwright_browser import (
    Browser,
    get_browser_config,
    get_playwright_proxy
)


logger = get_logger(__name__)


class ImageProcessor:
    """å›¾ç‰‡å¤„ç†å™¨ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶ã€ç½‘ç»œURLåŠæµè§ˆå™¨ä¸‹è½½"""
    
    def __init__(self, temp_dir: Optional[str] = None, cookies: Optional[List] = None):
        """
        åˆå§‹åŒ–å›¾ç‰‡å¤„ç†å™¨
        
        Args:
            temp_dir: ä¸´æ—¶æ–‡ä»¶ç›®å½•è·¯å¾„
            cookies: æµè§ˆå™¨cookiesï¼Œç”¨äºä¸‹è½½éœ€è¦ç™»å½•çš„å›¾ç‰‡
        """
        # è®¾ç½®ä¸´æ—¶ç›®å½•
        if temp_dir:
            self.temp_dir = Path(temp_dir)
        else:
            # ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
            temp_base = tempfile.gettempdir() or "/tmp"
            self.temp_dir = Path(temp_base) / "xhs_images"
        
        self.temp_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜cookiesç”¨äºä¸‹è½½
        self.cookies = cookies or []
        
        logger.info(f"å›¾ç‰‡å¤„ç†å™¨åˆå§‹åŒ–ï¼Œä¸´æ—¶ç›®å½•: {self.temp_dir}, Cookiesæ•°é‡: {len(self.cookies)}")
    
    async def process_images(self, images_input: Union[str, List, None], strict_mode: bool = True) -> List[str]:
        """
        å¤„ç†å„ç§æ ¼å¼çš„å›¾ç‰‡è¾“å…¥ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        æ”¯æŒæ ¼å¼ï¼š
        - æœ¬åœ°è·¯å¾„: "/path/to/image.jpg"
        - ç½‘ç»œåœ°å€: "https://example.com/image.jpg"
        - æ··åˆåˆ—è¡¨: ["path.jpg", "https://..."]
        
        Args:
            images_input: å›¾ç‰‡è¾“å…¥ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼Œå¦‚æœä¸ºTrueï¼Œåˆ™æ‰€æœ‰å›¾ç‰‡å¿…é¡»ä¸‹è½½æˆåŠŸï¼Œå¦åˆ™æŠ›å‡ºé”™è¯¯
            
        Returns:
            List[str]: æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Raises:
            Exception: å½“strict_mode=Trueä¸”å›¾ç‰‡ä¸‹è½½ä¸å®Œæ•´æ—¶æŠ›å‡ºé”™è¯¯
        """
        if not images_input:
            return []
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        images_list = self._normalize_to_list(images_input)
        
        if not images_list:
            return []
        
        logger.info(f"ğŸ“¸ å¼€å§‹å¤„ç†å›¾ç‰‡ï¼Œæ€»æ•°: {len(images_list)} å¼ ï¼Œä¸¥æ ¼æ¨¡å¼: {strict_mode}")

        # æ”¹ä¸ºé¡ºåºä¸‹è½½ï¼Œé¿å…å› URLæ—¶æ•ˆæ€§æˆ–æœåŠ¡å™¨å¹¶å‘é™åˆ¶å¯¼è‡´å¤±è´¥
        processed_results: List[Union[str, None]] = []
        for i, img in enumerate(images_list):
            result = await self._process_single_image(img, i)
            processed_results.append(result)

        successful_downloads: List[str] = []
        failed_images: List[Tuple[int, str]] = []
        
        for i, result in enumerate(processed_results):
            if isinstance(result, str):
                successful_downloads.append(result)
                logger.info(f"âœ… å¤„ç†å›¾ç‰‡æˆåŠŸ [{i+1}/{len(images_list)}]: {result}")
            else: # result is None or Exception
                failed_images.append((i, images_list[i]))
                if isinstance(result, Exception):
                     logger.error(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿå¼‚å¸¸ [{i+1}/{len(images_list)}]: {images_list[i]} (é”™è¯¯: {str(result)})")
                else:
                     logger.error(f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ [{i+1}/{len(images_list)}]: {images_list[i]} (æœªè·å¾—æœ¬åœ°è·¯å¾„)")

        # æ£€æŸ¥ä¸‹è½½å®Œæ•´æ€§
        success_count = len(successful_downloads)
        total_count = len(images_list)
        
        logger.info(f"ğŸ“Š å›¾ç‰‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{total_count} å¼ ")
        
        # ä¸¥æ ¼æ¨¡å¼ä¸‹æ£€æŸ¥å®Œæ•´æ€§
        if strict_mode and success_count != total_count:
            error_msg = f"å›¾ç‰‡ä¸‹è½½ä¸å®Œæ•´ï¼éœ€è¦ {total_count} å¼ ï¼Œå®é™…ä¸‹è½½æˆåŠŸ {success_count} å¼ "
            if failed_images:
                error_msg += f"\nå¤±è´¥çš„å›¾ç‰‡:\n" + "\n".join([f"ç¬¬{idx+1}å¼ å›¾ç‰‡: {url}" for idx, url in failed_images])
            
            logger.error(f"âŒ {error_msg}")
            
            # æ¸…ç†å·²ä¸‹è½½çš„ä¸´æ—¶æ–‡ä»¶
            await self._cleanup_downloaded_files(successful_downloads)
            
            raise Exception(error_msg)
        
        if success_count != total_count:
            logger.warning(f"âš ï¸ éƒ¨åˆ†å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œï¼ˆéä¸¥æ ¼æ¨¡å¼ï¼‰")
        
        return successful_downloads
    
    def _normalize_to_list(self, images_input: Union[str, List]) -> List:
        """å°†å„ç§è¾“å…¥æ ¼å¼ç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨"""
        if isinstance(images_input, str):
            # å•ä¸ªå­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯è·¯å¾„æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªè·¯å¾„
            if ',' in images_input:
                # é€—å·åˆ†éš”çš„å¤šä¸ªè·¯å¾„
                return [img.strip() for img in images_input.split(',') if img.strip()]
            else:
                return [images_input]
        elif isinstance(images_input, list):
            return images_input
        else:
            # å…¶ä»–ç±»å‹ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(images_input)}")
            return []
    
    async def _process_single_image(self, img_input: str, index: int) -> Optional[str]:
        """
        å¤„ç†å•ä¸ªå›¾ç‰‡è¾“å…¥
        
        Args:
            img_input: å›¾ç‰‡è¾“å…¥ï¼ˆå­—ç¬¦ä¸²ï¼‰
            index: å›¾ç‰‡ç´¢å¼•
            
        Returns:
            Optional[str]: æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        if not isinstance(img_input, str):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å›¾ç‰‡è¾“å…¥ç±»å‹: {type(img_input)}")
            return None
            
        # å¦‚æœæ˜¯ç½‘ç»œåœ°å€ï¼Œæ”¹ç”¨æµè§ˆå™¨ä¸‹è½½ï¼Œç¡®ä¿æˆåŠŸç‡
        if img_input.startswith(('http://', 'https://')):
            return await self._download_with_browser(img_input, index)
        elif os.path.exists(img_input):
            # æœ¬åœ°æ–‡ä»¶
            return os.path.abspath(img_input)
        else:
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å›¾ç‰‡è·¯å¾„: {img_input}")
            return None
    
    async def _download_with_browser(self, url: str, index: int) -> Optional[str]:
        """ä½¿ç”¨Playwrightæµè§ˆå™¨ä¸‹è½½å›¾ç‰‡ï¼Œè§£å†³å¤æ‚é˜²ç›—é“¾é—®é¢˜"""
        logger.info(f"ğŸš€ å°è¯•ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½: {url}")
        browser_instance = None
        try:
            config = get_browser_config()
            browser_instance = await Browser.create(
                config=config,
                proxy=get_playwright_proxy(config.proxy),
                cookies=self.cookies
            )
            page = await browser_instance.new_page()
            
            # å¯¼èˆªåˆ°å›¾ç‰‡URL
            response = await page.goto(url, wait_until="networkidle", timeout=120000)
            
            if response is None or not response.ok:
                logger.error(f"âŒ æµè§ˆå™¨å¯¼èˆªå¤±è´¥: {url}, çŠ¶æ€: {response.status if response else 'N/A'}")
                await browser_instance.close()
                return None

            # è·å–å›¾ç‰‡å†…å®¹
            image_bytes = await response.body()
            if not image_bytes:
                logger.error(f"âŒ æœªèƒ½ä»æµè§ˆå™¨è·å–å›¾ç‰‡å†…å®¹: {url}")
                await browser_instance.close()
                return None

            # ä»å“åº”å¤´è·å–æ–‡ä»¶ç±»å‹ï¼Œå¦‚æœå¤±è´¥åˆ™ä»URLçŒœæµ‹
            content_type = response.headers.get('content-type', '')
            ext = self._get_extension_from_content_type(content_type)
            if not ext:
                url_path = Path(url.split('?')[0])
                ext = url_path.suffix if url_path.suffix else ".png"

            filename = f"browser_download_{index}_{uuid.uuid4().hex[:8]}{ext}"
            filepath = self.temp_dir / filename
            filepath.write_bytes(image_bytes)
            
            logger.info(f"âœ… æµè§ˆå™¨ä¸‹è½½æˆåŠŸ: {url} -> {filepath}")
            await browser_instance.close()
            return str(filepath)

        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨ä¸‹è½½å¼‚å¸¸: {url}, é”™è¯¯: {e}")
            if browser_instance:
                await browser_instance.close()
            return None

    async def _download_from_url(self, url: str, index: int) -> Optional[str]:
        """
        ä¸‹è½½ç½‘ç»œå›¾ç‰‡åˆ°æœ¬åœ°ï¼ˆHTTP GETè¯·æ±‚ï¼Œä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            index: å›¾ç‰‡ç´¢å¼•
            
        Returns:
            Optional[str]: æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    await asyncio.sleep(1 * attempt)  # é‡è¯•å»¶è¿Ÿ
                    logger.info(f"ğŸ”„ é‡è¯•HTTPä¸‹è½½ (ç¬¬{attempt+1}æ¬¡): {url}")
                else:
                    logger.info(f"â¬‡ï¸ å°è¯•HTTPç›´æ¥ä¸‹è½½: {url}")
                
                # ä¸ºHTTPä¸‹è½½è®¾ç½®ä¸€ä¸ªé€šç”¨çš„è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
                    'Accept': 'image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                }

                # è®¾ç½®cookies jar (ä»…å¯¹å°çº¢ä¹¦åŸŸåä½¿ç”¨)
                jar = aiohttp.CookieJar()
                if self.cookies and 'xiaohongshu.com' in url:
                    logger.debug(f"ğŸª ä½¿ç”¨ {len(self.cookies)} ä¸ªcookiesä¸‹è½½å›¾ç‰‡")
                    for cookie in self.cookies:
                        try:
                            domain = cookie.get('domain', '.xiaohongshu.com')
                            if domain.startswith('.'):
                                domain = domain[1:]
                            
                            from yarl import URL
                            jar.update_cookies({
                                cookie['name']: cookie['value']
                            }, response_url=URL(f"https://{domain}"))
                        except Exception as e:
                            logger.debug(f"å¤„ç†cookieå¤±è´¥: {e}")
                
                # åˆ›å»ºè¿æ¥å™¨ï¼Œå¿½ç•¥SSLéªŒè¯ï¼ˆæŸäº›å›¾åºŠå¯èƒ½æœ‰SSLé—®é¢˜ï¼‰
                connector = aiohttp.TCPConnector(ssl=False, limit=10)
                
                async with aiohttp.ClientSession(headers=headers, cookie_jar=jar, connector=connector) as session:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=120)) as response:
                        if response.status != 200:
                            if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥
                                logger.error(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}, çŠ¶æ€ç : {response.status}")
                                return None
                            else:
                                logger.warning(f"âš ï¸ ä¸‹è½½å›¾ç‰‡å¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {url}, çŠ¶æ€ç : {response.status}, å‡†å¤‡é‡è¯•...")
                                continue
                        
                        # è·å–æ–‡ä»¶æ‰©å±•å
                        content_type = response.headers.get('content-type', '')
                        ext = self._get_extension_from_content_type(content_type)
                        if not ext:
                            # ä»URLä¸­å°è¯•è·å–æ‰©å±•å
                            url_path = Path(url.split('?')[0])
                            ext = url_path.suffix or '.jpg'
                        
                        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                        filename = f"download_{index}_{uuid.uuid4().hex[:8]}{ext}"
                        filepath = self.temp_dir / filename
                        
                        # ä¿å­˜æ–‡ä»¶
                        content = await response.read()
                        filepath.write_bytes(content)
                        
                        logger.info(f"âœ… ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {url} -> {filepath}")
                        return str(filepath)
                        
            except asyncio.TimeoutError:
                if attempt == max_retries - 1:
                    raise Exception(f"ä¸‹è½½å›¾ç‰‡è¶…æ—¶: {url}")
                else:
                    logger.warning(f"âš ï¸ ä¸‹è½½å›¾ç‰‡è¶…æ—¶ (ç¬¬{attempt+1}æ¬¡): {url}, å‡†å¤‡é‡è¯•...")
                    continue
            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
                else:
                    logger.warning(f"âš ï¸ ä¸‹è½½å›¾ç‰‡å¼‚å¸¸ (ç¬¬{attempt+1}æ¬¡): {url}, é”™è¯¯: {str(e)}, å‡†å¤‡é‡è¯•...")
                    continue
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return None
    
    def _get_extension_from_content_type(self, content_type: str) -> str:
        """æ ¹æ®content-typeè·å–æ–‡ä»¶æ‰©å±•å"""
        mapping = {
            'image/jpeg': '.jpg',
            'image/jpg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp'
        }
        
        # æå–ä¸»è¦çš„å†…å®¹ç±»å‹ï¼ˆå»é™¤å‚æ•°ï¼‰
        main_type = content_type.split(';')[0].strip().lower()
        return mapping.get(main_type, '')
    
    async def _cleanup_downloaded_files(self, file_paths: List[str]) -> None:
        """
        æ¸…ç†ä¸‹è½½å¤±è´¥æ—¶çš„ä¸´æ—¶æ–‡ä»¶
        
        Args:
            file_paths: è¦æ¸…ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not file_paths:
            return
        
        cleaned_count = 0
        for file_path in file_paths:
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    cleaned_count += 1
                    logger.debug(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        
        if cleaned_count > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
    
    def cleanup_old_files(self, max_age_hours: int = 24):
        """
        æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„ä¸´æ—¶æ–‡ä»¶
        
        Args:
            max_age_hours: æ–‡ä»¶æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        import time
        current_time = time.time()
        cleaned_count = 0
        
        try:
            for file in self.temp_dir.iterdir():
                if file.is_file():
                    file_age_hours = (current_time - file.stat().st_mtime) / 3600
                    if file_age_hours > max_age_hours:
                        try:
                            file.unlink()
                            cleaned_count += 1
                        except Exception as e:
                            logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
            
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å‡ºé”™: {e}")