"""
CSVå­˜å‚¨å®ç°

æä¾›åŸºäºCSVæ–‡ä»¶çš„æ•°æ®å­˜å‚¨åŠŸèƒ½
"""

import csv
import json
import logging
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from .base import BaseStorage

logger = logging.getLogger(__name__)


class CSVStorage(BaseStorage):
    """CSVå­˜å‚¨å®ç°ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–CSVå­˜å‚¨
        
        Args:
            config: é…ç½®å‚æ•°ï¼ŒåŒ…å«data_dirç­‰
        """
        super().__init__(config)
        self.data_dir = Path(config.get('data_dir', 'src/data'))
        self.csv_dir = self.data_dir / 'creator_db'
        
        # CSVæ–‡ä»¶è·¯å¾„
        self.dashboard_file = self.csv_dir / 'dashboard_data.csv'
        self.content_analysis_file = self.csv_dir / 'content_analysis_data.csv'
        self.fans_file = self.csv_dir / 'fans_data.csv'
        
        # CSVå­—æ®µå®šä¹‰ï¼ˆè‹±æ–‡å­—æ®µåï¼Œç”¨äºä»£ç é€»è¾‘å’Œæ•°æ®åº“ï¼‰
        self.dashboard_fields = [
            'created_at', 'updated_at', 'timestamp', 'dimension',
            'views', 'likes', 'collects', 'comments', 'shares', 'interactions'
        ]
        self.content_analysis_fields = [
            'created_at', 'updated_at', 'timestamp', 'title', 'note_type', 'publish_time',
            'views', 'likes', 'comments', 'collects', 'shares', 'fans_growth', 'avg_watch_time', 'danmu_count',
            # è§‚ä¼—æ¥æºæ•°æ®
            'source_recommend', 'source_search', 'source_follow', 'source_other',
            # è§‚ä¼—åˆ†ææ•°æ®
            'gender_male', 'gender_female', 'age_18_24', 'age_25_34', 'age_35_44', 'age_45_plus',
            'city_top1', 'city_top2', 'city_top3', 'interest_top1', 'interest_top2', 'interest_top3'
        ]
        self.fans_fields = [
            'created_at', 'updated_at', 'timestamp', 'dimension', 'total_fans', 'new_fans', 'lost_fans'
        ]
        
        # ä¸­æ–‡è¡¨å¤´æ˜ å°„ï¼ˆç”¨äºCSVæ–‡ä»¶æ˜¾ç¤ºï¼‰
        self.field_chinese_mapping = {
            # é€šç”¨å­—æ®µ
            'created_at': 'åˆ›å»ºæ—¶é—´',
            'updated_at': 'æ›´æ–°æ—¶é—´', 
            'timestamp': 'æ—¶é—´æˆ³',
            'dimension': 'ç»Ÿè®¡ç»´åº¦',
            
            # ä»ªè¡¨æ¿å­—æ®µ
            'views': 'æµè§ˆé‡',
            'likes': 'ç‚¹èµæ•°',
            'collects': 'æ”¶è—æ•°',
            'comments': 'è¯„è®ºæ•°',
            'shares': 'åˆ†äº«æ•°',
            'interactions': 'äº’åŠ¨æ•°',
            
            # å†…å®¹åˆ†æå­—æ®µ
            'title': 'ç¬”è®°æ ‡é¢˜',
            'note_type': 'ç¬”è®°ç±»å‹',
            'publish_time': 'å‘å¸ƒæ—¶é—´',
            'fans_growth': 'æ¶¨ç²‰æ•°',
            'avg_watch_time': 'å¹³å‡è§‚çœ‹æ—¶é•¿',
            'danmu_count': 'å¼¹å¹•æ•°',
            
            # è§‚ä¼—æ¥æºæ•°æ®
            'source_recommend': 'æ¨èæ¥æºå æ¯”',
            'source_search': 'æœç´¢æ¥æºå æ¯”',
            'source_follow': 'å…³æ³¨æ¥æºå æ¯”',
            'source_other': 'å…¶ä»–æ¥æºå æ¯”',
            
            # è§‚ä¼—åˆ†ææ•°æ®
            'gender_male': 'ç”·æ€§å æ¯”',
            'gender_female': 'å¥³æ€§å æ¯”',
            'age_18_24': '18-24å²å æ¯”',
            'age_25_34': '25-34å²å æ¯”',
            'age_35_44': '35-44å²å æ¯”',
            'age_45_plus': '45å²ä»¥ä¸Šå æ¯”',
            'city_top1': 'åŸå¸‚TOP1',
            'city_top2': 'åŸå¸‚TOP2',
            'city_top3': 'åŸå¸‚TOP3',
            'interest_top1': 'å…´è¶£TOP1',
            'interest_top2': 'å…´è¶£TOP2',
            'interest_top3': 'å…´è¶£TOP3',
            
            # ç²‰ä¸æ•°æ®å­—æ®µ
            'total_fans': 'æ€»ç²‰ä¸æ•°',
            'new_fans': 'æ–°å¢ç²‰ä¸',
            'lost_fans': 'æµå¤±ç²‰ä¸'
        }
        
        # ç”Ÿæˆä¸­æ–‡è¡¨å¤´åˆ—è¡¨
        self.dashboard_chinese_headers = [self.field_chinese_mapping[field] for field in self.dashboard_fields]
        self.content_analysis_chinese_headers = [self.field_chinese_mapping[field] for field in self.content_analysis_fields]
        self.fans_chinese_headers = [self.field_chinese_mapping[field] for field in self.fans_fields]
        
        # è‡ªåŠ¨åˆå§‹åŒ–
        self._initialize_sync()
    
    def _initialize_sync(self) -> None:
        """åŒæ­¥åˆå§‹åŒ–CSVå­˜å‚¨"""
        try:
            # åˆ›å»ºæ•°æ®ç›®å½•
            self.csv_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆå§‹åŒ–CSVæ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            self._init_csv_file(self.dashboard_file, self.dashboard_fields, self.dashboard_chinese_headers)
            self._init_csv_file(self.content_analysis_file, self.content_analysis_fields, self.content_analysis_chinese_headers)
            self._init_csv_file(self.fans_file, self.fans_fields, self.fans_chinese_headers)
            
            self._initialized = True
            logger.info(f"ğŸ“ CSVå­˜å‚¨åˆå§‹åŒ–æˆåŠŸï¼Œæ•°æ®ç›®å½•: {self.csv_dir}")
            
        except Exception as e:
            logger.error(f"âŒ CSVå­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def initialize(self) -> bool:
        """
        å¼‚æ­¥åˆå§‹åŒ–CSVå­˜å‚¨ï¼Œåˆ›å»ºå¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            self._initialize_sync()
            return True
            
        except Exception as e:
            logger.error(f"âŒ CSVå­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _init_csv_file(self, file_path: Path, fields: List[str], chinese_headers: List[str] = None) -> None:
        """
        åˆå§‹åŒ–CSVæ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
        
        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            fields: è‹±æ–‡å­—æ®µåˆ—è¡¨ï¼ˆç”¨äºä»£ç é€»è¾‘ï¼‰
            chinese_headers: ä¸­æ–‡è¡¨å¤´åˆ—è¡¨ï¼ˆç”¨äºCSVæ˜¾ç¤ºï¼‰
        """
        if not file_path.exists():
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                if chinese_headers:
                    # ä½¿ç”¨ä¸­æ–‡è¡¨å¤´
                    f.write(','.join(chinese_headers) + '\n')
                else:
                    # é™çº§åˆ°è‹±æ–‡è¡¨å¤´
                    writer = csv.DictWriter(f, fieldnames=fields)
                    writer.writeheader()
            logger.debug(f"ğŸ“„ åˆ›å»ºCSVæ–‡ä»¶: {file_path}")
    
    def save_dashboard_data(self, data: List[Dict[str, Any]]) -> None:
        """
        åŒæ­¥ä¿å­˜ä»ªè¡¨æ¿æ•°æ®åˆ°CSV
        
        Args:
            data: ä»ªè¡¨æ¿æ•°æ®åˆ—è¡¨
        """
        try:
            if not self._initialized:
                self._initialize_sync()
            
            # å¤„ç†æ•°æ®æ ¼å¼
            rows = []
            for item in data:
                row = self._add_timestamp({
                    'timestamp': item.get('timestamp', ''),
                    'dimension': item.get('dimension', ''),
                    'views': item.get('views', 0),
                    'likes': item.get('likes', 0),
                    'collects': item.get('collects', 0),
                    'comments': item.get('comments', 0),
                    'shares': item.get('shares', 0),
                    'interactions': item.get('interactions', 0)
                })
                rows.append(row)
            
            # æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜
            self._save_with_daily_overwrite(self.dashboard_file, self.dashboard_fields, rows, self.dashboard_chinese_headers)
            
            logger.info(f"ğŸ’¾ ä»ªè¡¨æ¿æ•°æ®å·²ä¿å­˜åˆ°CSV: {len(rows)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»ªè¡¨æ¿æ•°æ®å¤±è´¥: {e}")
            raise
    
    def save_content_analysis_data(self, data: List[Dict[str, Any]]) -> None:
        """
        åŒæ­¥ä¿å­˜å†…å®¹åˆ†ææ•°æ®åˆ°CSV
        
        Args:
            data: å†…å®¹åˆ†ææ•°æ®åˆ—è¡¨
        """
        try:
            if not self._initialized:
                self._initialize_sync()
            
            # å¤„ç†æ•°æ®æ ¼å¼
            rows = []
            for item in data:
                row = self._add_timestamp({
                    'timestamp': item.get('timestamp', ''),
                    'title': item.get('title', ''),
                    'note_type': item.get('note_type', ''),
                    'publish_time': item.get('publish_time', ''),
                    'views': item.get('views', 0),
                    'likes': item.get('likes', 0),
                    'comments': item.get('comments', 0),
                    'collects': item.get('collects', 0),
                    'shares': item.get('shares', 0),
                    'fans_growth': item.get('fans_growth', 0),
                    'avg_watch_time': item.get('avg_watch_time', ''),
                    'danmu_count': item.get('danmu_count', 0),
                    # è§‚ä¼—æ¥æºæ•°æ®
                    'source_recommend': item.get('source_recommend', '0%'),
                    'source_search': item.get('source_search', '0%'),
                    'source_follow': item.get('source_follow', '0%'),
                    'source_other': item.get('source_other', '0%'),
                    # è§‚ä¼—åˆ†ææ•°æ®
                    'gender_male': item.get('gender_male', '0%'),
                    'gender_female': item.get('gender_female', '0%'),
                    'age_18_24': item.get('age_18_24', '0%'),
                    'age_25_34': item.get('age_25_34', '0%'),
                    'age_35_44': item.get('age_35_44', '0%'),
                    'age_45_plus': item.get('age_45_plus', '0%'),
                    'city_top1': item.get('city_top1', ''),
                    'city_top2': item.get('city_top2', ''),
                    'city_top3': item.get('city_top3', ''),
                    'interest_top1': item.get('interest_top1', ''),
                    'interest_top2': item.get('interest_top2', ''),
                    'interest_top3': item.get('interest_top3', '')
                })
                rows.append(row)
            
            # æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜
            self._save_with_daily_overwrite(self.content_analysis_file, self.content_analysis_fields, rows, self.content_analysis_chinese_headers)
            
            logger.info(f"ğŸ’¾ å†…å®¹åˆ†ææ•°æ®å·²ä¿å­˜åˆ°CSV: {len(rows)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å†…å®¹åˆ†ææ•°æ®å¤±è´¥: {e}")
            raise
    
    def save_fans_data(self, data: List[Dict[str, Any]]) -> None:
        """
        åŒæ­¥ä¿å­˜ç²‰ä¸æ•°æ®åˆ°CSV
        
        Args:
            data: ç²‰ä¸æ•°æ®åˆ—è¡¨
        """
        try:
            if not self._initialized:
                self._initialize_sync()
            
            # å¤„ç†æ•°æ®æ ¼å¼
            rows = []
            for item in data:
                row = self._add_timestamp({
                    'timestamp': item.get('timestamp', ''),
                    'dimension': item.get('dimension', ''),
                    'total_fans': item.get('total_fans', 0),
                    'new_fans': item.get('new_fans', 0),
                    'lost_fans': item.get('lost_fans', 0)
                })
                rows.append(row)
            
            # æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜
            self._save_with_daily_overwrite(self.fans_file, self.fans_fields, rows, self.fans_chinese_headers)
            
            logger.info(f"ğŸ’¾ ç²‰ä¸æ•°æ®å·²ä¿å­˜åˆ°CSV: {len(rows)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç²‰ä¸æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def get_latest_data(self, data_type: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–æœ€æ–°æ•°æ®
        
        Args:
            data_type: æ•°æ®ç±»å‹ (dashboard, content_analysis, fans)
            limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
            
        Returns:
            List[Dict[str, Any]]: æ•°æ®åˆ—è¡¨
        """
        try:
            if data_type == 'dashboard':
                file_path = self.dashboard_file
                fields = self.dashboard_fields
                chinese_headers = self.dashboard_chinese_headers
            elif data_type == 'content_analysis':
                file_path = self.content_analysis_file
                fields = self.content_analysis_fields
                chinese_headers = self.content_analysis_chinese_headers
            elif data_type == 'fans':
                file_path = self.fans_file
                fields = self.fans_fields
                chinese_headers = self.fans_chinese_headers
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥æ•°æ®ç±»å‹: {data_type}")
                return []
            
            if not file_path.exists():
                return []
            
            # è¯»å–CSVæ–‡ä»¶
            data = []
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                headers = next(reader, None)  # è¯»å–è¡¨å¤´
                
                if headers:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡è¡¨å¤´
                    if headers == chinese_headers:
                        # ä¸­æ–‡è¡¨å¤´ï¼Œéœ€è¦è½¬æ¢ä¸ºè‹±æ–‡å­—æ®µå
                        for row in reader:
                            if len(row) == len(fields):
                                row_dict = {field: value for field, value in zip(fields, row)}
                                data.append(row_dict)
                    else:
                        # è‹±æ–‡è¡¨å¤´æˆ–å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                        for row in reader:
                            if len(row) == len(headers):
                                row_dict = {header: value for header, value in zip(headers, row)}
                                data.append(row_dict)
            
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—ï¼Œè¿”å›æœ€æ–°çš„limitæ¡
            data.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            return data[:limit]
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def close(self) -> None:
        """å…³é—­å­˜å‚¨è¿æ¥"""
        logger.debug("ğŸ“ CSVå­˜å‚¨è¿æ¥å·²å…³é—­")
    
    def get_storage_info(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ä¿¡æ¯"""
        try:
            info = {
                'storage_type': 'CSV',
                'data_path': str(self.data_dir),
                'csv_path': str(self.csv_dir),
                'initialized': self._initialized,
                'files': {}
            }
            
            # æ£€æŸ¥å„ä¸ªCSVæ–‡ä»¶çš„çŠ¶æ€
            files = {
                'dashboard_data.csv': self.dashboard_file,
                'content_analysis_data.csv': self.content_analysis_file,
                'fans_data.csv': self.fans_file
            }
            
            for name, path in files.items():
                if path.exists():
                    # ç»Ÿè®¡è®°å½•æ•°ï¼ˆå‡å»è¡¨å¤´ï¼‰
                    with open(path, 'r', encoding='utf-8') as f:
                        reader = csv.reader(f)
                        row_count = sum(1 for _ in reader) - 1  # å‡å»è¡¨å¤´
                    
                    info['files'][name] = {
                        'exists': True,
                        'path': str(path),
                        'records': max(0, row_count),
                        'size_bytes': path.stat().st_size
                    }
                else:
                    info['files'][name] = {
                        'exists': False,
                        'path': str(path),
                        'records': 0,
                        'size_bytes': 0
                    }
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ è·å–å­˜å‚¨ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'storage_type': 'CSV',
                'error': str(e)
            }
    
    def _add_timestamp(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºæ•°æ®æ·»åŠ æ—¶é—´æˆ³
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            æ·»åŠ æ—¶é—´æˆ³çš„æ•°æ®
        """
        now = datetime.now().isoformat()
        data['created_at'] = now
        data['updated_at'] = now
        return data
    
    def _get_today_date(self) -> str:
        """è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²"""
        return datetime.now().strftime('%Y-%m-%d')
    
    def _save_with_daily_overwrite(self, file_path: Path, fields: List[str], new_data: List[Dict[str, Any]], chinese_headers: List[str] = None) -> None:
        """
        æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜æ•°æ®
        
        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            fields: è‹±æ–‡å­—æ®µåˆ—è¡¨
            new_data: æ–°æ•°æ®åˆ—è¡¨
            chinese_headers: ä¸­æ–‡è¡¨å¤´åˆ—è¡¨
        """
        try:
            today = self._get_today_date()
            
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path)
                    if not df.empty:
                        # å¦‚æœCSVä½¿ç”¨ä¸­æ–‡è¡¨å¤´ï¼Œéœ€è¦è½¬æ¢ä¸ºè‹±æ–‡å­—æ®µå
                        if chinese_headers and len(df.columns) == len(chinese_headers):
                            # åˆ›å»ºä¸­æ–‡åˆ°è‹±æ–‡çš„æ˜ å°„
                            chinese_to_english = {chinese: english for chinese, english in zip(chinese_headers, fields)}
                            df.rename(columns=chinese_to_english, inplace=True)
                        
                        # è¿‡æ»¤æ‰ä»Šå¤©çš„æ•°æ®
                        if 'created_at' in df.columns:
                            df['date'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d')
                            df_filtered = df[df['date'] != today]
                            existing_data = df_filtered.drop('date', axis=1).to_dict('records')
                        else:
                            existing_data = df.to_dict('records')
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–ç°æœ‰CSVæ•°æ®å¤±è´¥: {e}")
            
            # åˆå¹¶æ•°æ®ï¼šä¿ç•™éä»Šå¤©çš„æ•°æ® + ä»Šå¤©çš„æ–°æ•°æ®
            all_data = existing_data + new_data
            
            # é‡å†™æ•´ä¸ªæ–‡ä»¶
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                if chinese_headers:
                    # ä½¿ç”¨ä¸­æ–‡è¡¨å¤´
                    f.write(','.join(chinese_headers) + '\n')
                    # å†™å…¥æ•°æ®è¡Œï¼ŒæŒ‰å­—æ®µé¡ºåº
                    for row in all_data:
                        values = [str(row.get(field, '')) for field in fields]
                        f.write(','.join(values) + '\n')
                else:
                    # é™çº§åˆ°è‹±æ–‡è¡¨å¤´
                    writer = csv.DictWriter(f, fieldnames=fields)
                    writer.writeheader()
                    writer.writerows(all_data)
            
            logger.info(f"ğŸ’¾ æ•°æ®å·²æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜: ä¿ç•™ {len(existing_data)} æ¡å†å²è®°å½•ï¼Œæ–°å¢ {len(new_data)} æ¡ä»Šæ—¥è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰æ—¥æœŸè¦†ç›–ä¿å­˜å¤±è´¥: {e}")
            # é™çº§åˆ°è¿½åŠ æ¨¡å¼
            self._append_to_csv(file_path, fields, new_data)
    
    def _append_to_csv(self, file_path: Path, fields: List[str], data: List[Dict[str, Any]]) -> None:
        """
        è¿½åŠ æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            fields: å­—æ®µåˆ—è¡¨
            data: æ•°æ®åˆ—è¡¨
        """
        try:
            with open(file_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fields)
                writer.writerows(data)
            logger.info(f"ğŸ’¾ æ•°æ®å·²è¿½åŠ ä¿å­˜: {len(data)} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"âŒ è¿½åŠ ä¿å­˜å¤±è´¥: {e}")
            raise 